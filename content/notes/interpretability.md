---
type: note
baseurl: /notes/
fullurl: " "
logo: " "
hero: " "
course: mli
title: Interpretability
slug: interpretability
topic: 7.2
hidden: false
tags:
  - Interpretability
  - Trustworthy AI/ML
---

## Why is interpretability important?

- <mark>Complexity and prevalence</mark>
- <mark>Safety</mark> (e.g. medical diagnosis) and robustness (e.g. self driving cars) is critical
- Generating knowledge
- <mark>Debugging ML</mark> - why does something not train/exhibit unexpected behaviour
- To use ML <mark>responsibly</mark>, we need to ensure:
  - Our values are aligned
  - Our knowledge is reflected (e.g. <mark>detecting bias</mark>)

## Common Misunderstandings

- <mark>Simple ML models are easy to interpret</mark>, such as decision trees (these get hard to interpret when they get big)
- <mark>Trust, fairness and interpretability are the same thing</mark>
  - Interpretability is a key ingredient in tryst, fairness, accountability and causality, but not the other way around
  - It's used to help formalise these concepts
- We don't always care about it
  - If there are <mark>no significant consequences</mark>, or when you just need predictions
  - Sufficiently <mark>well-studied problems</mark>
  - <mark>Prevent gaming the system</mark> (so we want it to be uninterpretable)

## How to interpret existing ML models

### <mark>Ablation Test</mark>

This tests <mark>how important</mark> a data point or feature is

You <mark>train a model without certain features</mark> and <mark>observe the impact</mark>

However, this is <mark>difficult and expensive</mark>

### <mark>Fit Functions</mark>

This is where you use the <mark>first derivatives</mark> to do a <mark>sensitivity analysis</mark>

This is how quickly a functions is changing around certain data points

This is a <mark>local analysis</mark>

### Visualisation and attribution

This is where you <mark>identify input features responsible for a model decision</mark>

A direct visualisation of the features is easy to implement, but limited in practical value

The <mark>first layers are easy to interpret</mark> (mostly low level features) but higher layers are more difficult, as they are more abstract

The problem is that visualisation of filters has limited value, therefore we instead visualise activations generated by kernels

A strong response means a feature is present, and a weak one means it is not

This is also <mark>easy to implement</mark>, and easy to interpret for early layers

Higher layers are more sparse

Channels may correspond to specific features

### Occlusions

The ideas is to <mark>mask out regions</mark> in the input image and observe the network output

If a masked out region causes a significant <mark>drop in confidence</mark>, then it's <mark>important</mark>

### Saliency Maps

<mark>DeconvNet:</mark>

- Given a trained network and an image
- Choose activation at one layer (set all others to 0)
- Invert network
- No training involved
- Backward pass in network is almost identical to back propagation

It's used to <mark>visualise abstract learned features</mark>

It <mark>exaggerates the discriminative parts</mark> of the image

Question:

- Which pixels are most significant to a neuron?
- How would they need to change to most affect the activation of the neuron?

Solution:

- Use <mark>back propagation</mark> but <mark>differentiate activation with respect to input pixels</mark>, not weights, so the weights of the network are fixed
- This uses the <mark>activation function as a loss function</mark>

<mark>Gradient back propagation:</mark>

- Define loss as activation of arbitrary neuron in any layer
- The salient pixels with light up

<mark>Guided back propagation:</mark>

- This improves the results by "guiding" the back propagation process
- The idea is:
  - Positive gradients = features the neuron is interested in
  - Negative gradients = features the neuron is not interested in
  - Set all negative gradients in the back propagation to zero

### <mark>DeepDream</mark> / Inceptionism

This is an attempt to <mark>understand the inner workings of the network</mark>

You optimise with respect to the image

The ideas is:

- <mark>Arbitrary image or noise as input</mark>
- Instead of adjusting network parameters, <mark>tweak image towards high "X"</mark> where "X" can be:
  - Neuron/Activation map/layer
  - Logits/class probability
- Search for <mark>images that are "interesting"</mark>
- Different layers enhance different features

The algorithm is:

- Find an image $x$ such that the activation $\phi(x) at later $n\$ is high:
  - $\max\limits_x \phi_n(x) = \lambda \mathcal{R}(x)$
- Forward propagate to layer $n$]
- No minimisation of loss. Instead, maximise L2 norm of activations of a particular NN layer
- Backpropagate to input later
- Resulting image will show learned features

### <mark>Inversion</mark>

This attempts to <mark>construct an image from a layer activation</mark> $\hat{y}$

$\hat{x} = \min\limits_x (||\phi(x) - \hat{y}||_2^2 + \lambda \mathcal{R}(x))$

Where:

- $\hat{x}$ is the reconstructed image
- $\phi(x)$ is the network output for input image $x$
- $\hat{y}$ is the desired activation
- $\mathcal{R}$ is the regulariser

This will produce an image which, if we encode in the NN, will produce a similar activation to the activation we currently have

## Deep Networks as Encoders

Deep networks <mark>take an image $x$</mark> and <mark>reduce it to a code $y$ using transformation $\phi$</mark> (e.g. ImageNet takes an image and returns a 1000 dim code)

Since encoding performs dimensionality reduction, <mark>multiple images map to the same code</mark>, so it shouldn't be easily invertable

Therefore, <mark>reconstructions are not unique</mark>, but rather from an <mark>equivalence class</mark> that are the same for the network

If we apply a NN to an image in this equivalence class, the image should map to the class $y$

To find a pre-image, we need to solve the optimisation problem, where we start from random noise and match the code by optimisation

$\min\limits_x || \Phi(x) - \Phi(x_0)||^2$

Reconstructions should be <mark>constrained to natural images</mark> - not all images in an equivalence class look natural

Therefore we constrain our space to the space of natural looking images using a regularisation term

## Parameterising images via <mark>generative networks</mark>

Consider a generator network $\Psi$ with fixed input $z_0$

The network parameters $w$ can be thought of as a parameterisation of images

$w \rightarrow x = \Psi (z_o;w)$

You then <mark>adjust the weights $w$</mark> so the <mark>network $\Psi$ generates the desired output image $x$</mark>

To fit a network to a single image:

- Start with a <mark>random-initialised network</mark>
- Given an image $x$, its <mark>parameters $w$ are recovered</mark> by solving the optimisation problem $\min\limits_w ||x - \Psi(z_o;w)||^2$
- This is similar to learning a network from a single image

For most generative networks, fitting naturally looking images is easier/faster than fitting others, as MSE converges faster

For <mark>inpainting, we only reconstruct the visible pixels,</mark> implicitly inferring the pixels that are masked out by mask m $\min\limits_w ||m \bigodot( x - \Psi(z_o;w))||^2$

## Adversarial Methods

An image can be subject to an <mark>adversarial attack</mark>

This is where an <mark>image has a noise vector added</mark>, which <mark>isn't perceptible to the human eye</mark>, but <mark>throws off a classifier</mark>

### Pertubation

Assume a <mark>linear classifier</mark> $\theta^T x$

We can think of an adversarial example that contains a small, non-perceivable perturbation to the input, denoted as $\eta$, as $\tilde{x} = x + \eta$

Then, the logits of the classifier would be:

$\theta^T\tilde{x} = \theta^T(x + \eta) = \theta^Tx + \theta^T\eta$

Given the small perturbation $\eta$, the effect of the perturbation on the logts of the classfier is given by $\theta^T\eta$

The idea is to <mark>find an $\eta$ that causes a change that is non-percievable and ostensibly innocuous to the human eye, yet destructive and adverse enough for the classifier to the extent that its predictions are no longer accurate</mark>

An adversarial example is one that <mark>maximises the value of $\theta^T\eta$</mark> to sway the model into making a wrong prediction

We therefore need a constraint on $\eta$, otherwise someone could just apply a large, percievable perturbation

we can therefore apply the constraint $||\eta||_\infty \leq \epsilon$ and $||X||_\infty = \max\limits_i \{|x_i|\}$

Assume a perturbation $\eta = \epsilon \cdot sign(\theta)$

The bounds of this are:

$\eta = \epsilon \cdot sign(\theta)$

$\theta^T \eta = \epsilon \cdot \theta^T sign(\theta) = \$\epsilon||\theta||\\_1 = \epsilon mn$

Where the average magnitude of an element $\theta$ is given by $m$

This means that the <mark>change in activation given by the perturbation increases linearly with respect to $n$</mark> (num of pixels)

If <mark>$n$ is large</mark>, one can expect even a <mark>small perturbation capped at $\epsilon$</mark> to produce a perturbation <mark>big enough to render the model susceptible to an adversarial attack</mark>

These perturbed examples are referred to as <mark>adversarial examples</mark>

### Fast Gradient Sign Method

Here, the key idea is:

- Perform <mark>gradient descent in order to maximise the loss</mark> (we want to attack our model)
- Consider the <mark>input image $x$ to be a trainable parameter</mark> and compute the gradient with respect to the input image to create a perturbation
- $ \eta = \epsilon \cdot sign(\nabla_x\mathcal{L}(\theta,x,y))$
- An adversarial example cann be creates as:
  - $ \tilde{x} = x +  \epsilon \cdot sign(\nabla_x\mathcal{L}(\theta,x,y))$

### Adversarial data augmentation

How can use use adversarial attacks for <mark>defence</mark>?

1. <mark>Generate </mark>adversarial examples
2. Add the generated adversarial examples to the <mark>training set</mark>
3. <mark>Retrain</mark> the model using the training set
